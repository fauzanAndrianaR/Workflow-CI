name: MLflow Skilled CI

on:
  push:
    branches: [ "main", "develop" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.12.7'
  MLFLOW_TRACKING_URI: file:./mlruns
  MODEL_NAME: 'CI_RF_Model_Skilled'

jobs:
  validate-structure:
    runs-on: ubuntu-latest
    name: Validate Repository Structure
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate project structure
        run: |
          echo "🔍 Validating repository structure..."
          
          # Check required directories
          required_dirs=("MLProject" "MLProject/dataset_preprocessing")
          for dir in "${required_dirs[@]}"; do
            if [ ! -d "$dir" ]; then
              echo "❌ Missing required directory: $dir"
              exit 1
            else
              echo "✅ Directory exists: $dir"
            fi
          done
          
          # Check required files
          required_files=(
            "MLProject/modelling.py"
            "MLProject/conda.yaml" 
            "MLProject/requirements.txt"
          )
          
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Missing required file: $file"
              exit 1
            else
              echo "✅ File exists: $file"
            fi
          done
          
          echo "🎉 Repository structure validation passed!"

  setup-environment:
    runs-on: ubuntu-latest
    needs: validate-structure
    name: Setup Python Environment
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('MLProject/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Validate Python environment
        run: |
          echo "🐍 Python environment validation"
          python --version
          pip --version
          echo "Current directory: $(pwd)"
          echo "Available memory: $(free -h)"
          echo "CPU info: $(nproc) cores"

      - name: Install dependencies with error handling
        run: |
          echo "📦 Installing dependencies..."
          python -m pip install --upgrade pip setuptools wheel
          
          if [ -f "MLProject/requirements.txt" ]; then
            echo "Installing from requirements.txt..."
            pip install -r MLProject/requirements.txt
          else
            echo "❌ requirements.txt not found!"
            exit 1
          fi
          
          echo "✅ Dependencies installed successfully"
          echo "Installed packages:"
          pip list --format=columns

  data-validation:
    runs-on: ubuntu-latest
    needs: setup-environment
    name: Validate Training Data
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt

      - name: Validate dataset files
        run: |
          echo "🔍 Validating dataset files..."
          cd MLProject
          
          python -c "
          import os
          import pandas as pd
          import numpy as np
          
          # Check if preprocessing directory exists
          data_dir = './dataset_preprocessing'
          if not os.path.exists(data_dir):
              print('❌ Dataset preprocessing directory not found!')
              exit(1)
          
          # Required files
          required_files = ['X_train.csv', 'X_test.csv', 'y_train.csv', 'y_test.csv']
          
          for file in required_files:
              file_path = os.path.join(data_dir, file)
              if not os.path.exists(file_path):
                  print(f'❌ Missing file: {file}')
                  exit(1)
              else:
                  # Load and validate file
                  try:
                      df = pd.read_csv(file_path)
                      print(f'✅ {file}: Shape {df.shape}')
                      
                      # Check for missing values
                      if df.isnull().sum().sum() > 0:
                          print(f'⚠️  {file} contains missing values')
                      
                      # Check if file is empty
                      if df.empty:
                          print(f'❌ {file} is empty!')
                          exit(1)
                          
                  except Exception as e:
                      print(f'❌ Error reading {file}: {e}')
                      exit(1)
          
          print('🎉 Dataset validation completed successfully!')
          "

  model-training:
    runs-on: ubuntu-latest
    needs: data-validation
    name: Train ML Model
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt

      - name: Create MLflow tracking directory
        run: |
          mkdir -p MLProject/mlruns
          echo "📁 MLflow tracking directory created"

      - name: Execute model training
        run: |
          cd MLProject
          echo "🚀 Starting model training process..."
          
          # Set MLflow tracking URI
          export MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }}
          
          # Run the training script
          python modelling.py
          
          echo "✅ Model training completed"

      - name: Validate MLflow artifacts
        run: |
          cd MLProject
          echo "🔍 Validating MLflow artifacts..."
          
          if [ -d "mlruns" ]; then
            echo "✅ MLruns directory exists"
            echo "📊 MLflow artifacts structure:"
            find mlruns -name "*.json" | head -10
            find mlruns -name "*.png" | head -5
            find mlruns -name "*.pkl" | head -5
            
            # Get total size
            size=$(du -sh mlruns | cut -f1)
            echo "📏 Total MLflow artifacts size: $size"
          else
            echo "❌ MLruns directory not found!"
            exit 1
          fi

      - name: Extract model metrics
        id: metrics
        run: |
          cd MLProject
          echo "📈 Extracting model metrics..."
          
          python -c "
          import mlflow
          import os
          import json
          
          if os.path.exists('mlruns'):
              try:
                  # Get the latest run information
                  experiments = mlflow.search_experiments()
                  if experiments:
                      runs = mlflow.search_runs(experiment_ids=[experiments[0].experiment_id])
                      if not runs.empty:
                          latest_run = runs.iloc[0]
                          
                          metrics = {
                              'run_id': latest_run['run_id'],
                              'accuracy': float(latest_run['metrics.accuracy']) if 'metrics.accuracy' in latest_run else 0.0,
                              'train_size': int(latest_run['metrics.train_size']) if 'metrics.train_size' in latest_run else 0,
                              'test_size': int(latest_run['metrics.test_size']) if 'metrics.test_size' in latest_run else 0,
                              'feature_count': int(latest_run['metrics.feature_count']) if 'metrics.feature_count' in latest_run else 0
                          }
                          
                          print(f'Accuracy: {metrics[\"accuracy\"]:.4f}')
                          print(f'Training samples: {metrics[\"train_size\"]}')
                          print(f'Test samples: {metrics[\"test_size\"]}')
                          print(f'Features: {metrics[\"feature_count\"]}')
                          
                          # Save metrics to file for artifacts
                          with open('model_metrics.json', 'w') as f:
                              json.dump(metrics, f, indent=2)
                          
                          # Set output for GitHub Actions
                          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                              f.write(f'accuracy={metrics[\"accuracy\"]:.4f}\n')
                              f.write(f'run_id={metrics[\"run_id\"]}\n')
                      else:
                          print('No runs found')
                  else:
                      print('No experiments found')
              except Exception as e:
                  print(f'Error extracting metrics: {e}')
          else:
              print('MLruns directory not found')
          "

      - name: Generate training report
        run: |
          cd MLProject
          echo "📝 Generating training report..."
          
          cat > training_report.md << EOF
          # MLflow Training Report
          
          **Generated on:** $(date)
          **Commit SHA:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          **Workflow Run:** ${{ github.run_number }}
          
          ## Model Performance
          - **Accuracy:** ${{ steps.metrics.outputs.accuracy }}
          - **Run ID:** ${{ steps.metrics.outputs.run_id }}
          
          ## Environment Info
          - **Python Version:** ${{ env.PYTHON_VERSION }}
          - **MLflow Tracking URI:** ${{ env.MLFLOW_TRACKING_URI }}
          - **Model Name:** ${{ env.MODEL_NAME }}
          
          ## Artifacts Generated
          - Model artifacts stored in MLflow
          - Confusion matrix visualization
          - Model metrics and parameters
          
          EOF
          
          echo "✅ Training report generated"

      - name: Upload MLflow artifacts
        uses: actions/upload-artifact@v3
        with:
          name: mlflow-artifacts-${{ github.run_number }}
          path: |
            MLProject/mlruns/
            MLProject/model_metrics.json
            MLProject/training_report.md
          retention-days: 30

      - name: Upload model metrics
        uses: actions/upload-artifact@v3
        with:
          name: model-metrics-${{ github.run_number }}
          path: MLProject/model_metrics.json
          retention-days: 7

  model-validation:
    runs-on: ubuntu-latest
    needs: model-training
    name: Validate Trained Model
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r MLProject/requirements.txt

      - name: Download MLflow artifacts
        uses: actions/download-artifact@v3
        with:
          name: mlflow-artifacts-${{ github.run_number }}
          path: MLProject/

      - name: Validate model performance
        run: |
          cd MLProject
          echo "🔍 Validating model performance..."
          
          python -c "
          import json
          import os
          
          # Check if metrics file exists
          if os.path.exists('model_metrics.json'):
              with open('model_metrics.json', 'r') as f:
                  metrics = json.load(f)
              
              accuracy = metrics.get('accuracy', 0.0)
              print(f'Model accuracy: {accuracy:.4f}')
              
              # Set minimum acceptable accuracy
              min_accuracy = 0.60
              
              if accuracy >= min_accuracy:
                  print(f'✅ Model performance acceptable (>= {min_accuracy:.2f})')
              else:
                  print(f'❌ Model performance below threshold (< {min_accuracy:.2f})')
                  exit(1)
          else:
              print('❌ Model metrics file not found!')
              exit(1)
          "

  deploy-artifacts:
    runs-on: ubuntu-latest
    needs: model-validation
    name: Deploy Artifacts to Repository
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          name: mlflow-artifacts-${{ github.run_number }}
          path: artifacts/

      - name: Create artifacts directory structure
        run: |
          echo "📁 Creating artifacts directory structure..."
          mkdir -p model-artifacts/$(date +%Y-%m-%d)
          
          # Move artifacts to organized structure
          if [ -d "artifacts/mlruns" ]; then
            cp -r artifacts/mlruns model-artifacts/$(date +%Y-%m-%d)/
          fi
          
          if [ -f "artifacts/model_metrics.json" ]; then
            cp artifacts/model_metrics.json model-artifacts/$(date +%Y-%m-%d)/
          fi
          
          if [ -f "artifacts/training_report.md" ]; then
            cp artifacts/training_report.md model-artifacts/$(date +%Y-%m-%d)/
          fi
          
          # Create index file
          cat > model-artifacts/README.md << EOF
          # Model Artifacts Repository
          
          This directory contains MLflow model artifacts generated by CI/CD pipeline.
          
          ## Latest Artifacts
          - **Date:** $(date +%Y-%m-%d)
          - **Commit:** ${{ github.sha }}
          - **Workflow:** ${{ github.run_number }}
          
          ## Directory Structure
          - Each subdirectory represents artifacts from a specific training run
          - \`mlruns/\` contains MLflow tracking data
          - \`model_metrics.json\` contains model performance metrics
          - \`training_report.md\` contains detailed training report
          
          EOF

      - name: Commit and push artifacts
        run: |
          echo "🚀 Committing artifacts to repository..."
          
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add model-artifacts/
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "🤖 Add model artifacts from workflow run ${{ github.run_number }}
            
            - Commit: ${{ github.sha }}
            - Date: $(date)
            - Branch: ${{ github.ref_name }}
            "
            git push
            echo "✅ Artifacts committed and pushed successfully!"
          fi

  notify-completion:
    runs-on: ubuntu-latest
    needs: [validate-structure, setup-environment, data-validation, model-training, model-validation, deploy-artifacts]
    name: Notify Completion
    if: always()
    
    steps:
      - name: Check workflow status
        run: |
          echo "🏁 Workflow Summary"
          echo "=================="
          echo "Workflow: MLflow Skilled CI"
          echo "Run Number: ${{ github.run_number }}"
          echo "Commit: ${{ github.sha }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Triggered by: ${{ github.event_name }}"
          
          # Check job statuses
          if [[ "${{ needs.validate-structure.result }}" == "success" ]]; then
            echo "✅ Repository structure validation"
          else
            echo "❌ Repository structure validation"
          fi
          
          if [[ "${{ needs.setup-environment.result }}" == "success" ]]; then
            echo "✅ Environment setup"
          else
            echo "❌ Environment setup"
          fi
          
          if [[ "${{ needs.data-validation.result }}" == "success" ]]; then
            echo "✅ Data validation"
          else
            echo "❌ Data validation"
          fi
          
          if [[ "${{ needs.model-training.result }}" == "success" ]]; then
            echo "✅ Model training"
          else
            echo "❌ Model training"
          fi
          
          if [[ "${{ needs.model-validation.result }}" == "success" ]]; then
            echo "✅ Model validation"
          else
            echo "❌ Model validation"
          fi
          
          if [[ "${{ needs.deploy-artifacts.result }}" == "success" ]] || [[ "${{ needs.deploy-artifacts.result }}" == "skipped" ]]; then
            echo "✅ Artifact deployment"
          else
            echo "❌ Artifact deployment"
          fi
          
          echo "=================="
          echo "🎉 MLflow Skilled CI workflow completed!"